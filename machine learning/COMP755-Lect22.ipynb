{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "aab7fd3c-11a1-4073-a957-c10735d7afae"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# COMP 755\n",
    "\n",
    "Today:\n",
    "\n",
    "Fitting neural network's parameters using back-propagation\n",
    "\n",
    "\n",
    "$$\n",
    "\\renewcommand{\\vvec}[2]{\\left[ \\begin{array}{c} \\mathbf{#1}\\\\ \\mathbf{#2} \\end{array}\\right]}\n",
    "\\renewcommand{\\mmat}[4]{\\left[ \\begin{array}{cc} \\mathbf{#1}&\\mathbf{#2}\\\\ \\mathbf{#3}&\\mathbf{#4} \\end{array}\\right]}\n",
    "\\renewcommand{\\aaa}{\\mathbf{a}}\n",
    "\\renewcommand{\\AAA}{\\mathbf{A}}\n",
    "\\renewcommand{\\xyvec}{\\left[ \\begin{array}{c} \\xx\\\\\\yy \\end{array} \\right]}\n",
    "\\renewcommand{\\xyvecc}{\\left[ \\begin{array}{c} x^1\\\\y^1 \\end{array} \\right]}\n",
    "\\renewcommand{\\mm}{\\mathbf{m}}\n",
    "\\renewcommand{\\xx}{\\mathbf{x}}\n",
    "\\renewcommand{\\ff}{\\mathbf{f}}\n",
    "\\renewcommand{\\yy}{\\mathbf{y}}\n",
    "\\renewcommand{\\zz}{\\mathbf{z}}\n",
    "\\renewcommand{\\vv}{\\mathbf{v}}\n",
    "\\renewcommand{\\ee}{\\mathbf{e}}\n",
    "\\renewcommand{\\ww}{\\mathbf{w}}\n",
    "\\renewcommand{\\XX}{\\mathbf{X}}\n",
    "\\renewcommand{\\YY}{\\mathbf{Y}}\n",
    "\\renewcommand{\\WW}{\\mathbf{W}}\n",
    "\\renewcommand{\\VV}{\\mathbf{V}}\n",
    "\\renewcommand{\\DD}{\\mathbf{D}}\n",
    "\\renewcommand{\\dd}{\\mathbf{d}}\n",
    "\\renewcommand{\\ZZ}{\\mathbf{Z}}\n",
    "\\renewcommand{\\CC}{\\mathbf{C}}\n",
    "\\renewcommand{\\bbeta}{\\boldsymbol{\\mathbf{\\beta}}}\n",
    "\\renewcommand{\\ddelta}{\\boldsymbol{\\mathbf{\\delta}}}\n",
    "\\renewcommand{\\mmu}{\\boldsymbol{\\mathbf{\\mu}}}\n",
    "\\renewcommand{\\ssigma}{\\boldsymbol{\\mathbf{\\sigma}}}\n",
    "\\renewcommand{\\reals}{\\mathbb{R}}\n",
    "\\renewcommand{\\loglik}{\\mathcal{LL}}\n",
    "\\renewcommand{\\penloglik}{\\mathcal{PLL}}\n",
    "\\renewcommand{\\likelihood}{\\mathcal{L}}\n",
    "\\renewcommand{\\Data}{\\textrm{Data}}\n",
    "\\renewcommand{\\given}{ \\big| }\n",
    "\\renewcommand{\\MLE}{\\textrm{MLE}}\n",
    "\\renewcommand{\\EE}{\\mathbb{E}}\n",
    "\\renewcommand{\\EEE}{\\mathbf{E}}\n",
    "\\renewcommand{\\KL}{\\textrm{KL}}\n",
    "\\renewcommand{\\Bound}{\\mathcal{B}}\n",
    "\\renewcommand{\\tth}{\\textrm{th}}\n",
    "\\renewcommand{\\Gaussian}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\renewcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\renewcommand{\\ones}{\\mathbf{1}}\n",
    "\\renewcommand{\\corr}[2]{\\textrm{corr}(#1,#2)}\n",
    "\\renewcommand{\\diag}[1]{\\textrm{diag}\\left( #1 \\right)}\n",
    "\\renewcommand{\\sigmoid}[1]{\\sigma\\left(#1\\right)}\n",
    "\\renewcommand{\\myexp}[1]{\\exp\\left\\{#1\\right\\}}\n",
    "\\renewcommand{\\mylog}[1]{\\log\\left\\{#1\\right\\}}\n",
    "\\renewcommand{\\argmax}{\\mathop{\\textrm{argmax}}}\n",
    "\\renewcommand{\\new}{\\textrm{new}}\n",
    "\\renewcommand{\\old}{\\textrm{old}}\n",
    "\\renewcommand{\\bb}{\\mathbf{b}}\n",
    "\\renewcommand{\\ba}{\\mathbf{a}}\n",
    "\\renewcommand{\\bg}{\\mathbf{g}}\n",
    "\\renewcommand{\\BB}{\\mathbf{B}}\n",
    "\\renewcommand{\\BA}{\\mathbf{A}}\n",
    "\\renewcommand{\\BC}{\\mathbf{C}}\n",
    "\\renewcommand{\\UU}{\\mathbf{U}}\n",
    "\\renewcommand{\\uu}{\\mathbf{u}}\n",
    "\\renewcommand{\\hh}{\\mathbf{h}}\n",
    "\\renewcommand{\\SSS}{\\mathbf{S}}\n",
    "\\renewcommand{\\sss}{\\mathbf{s}}\n",
    "\\renewcommand{\\rr}{\\mathbf{r}}\n",
    "\\renewcommand{\\tr}[1]{\\textrm{tr}\\left\\{#1\\right\\}}\n",
    "\\renewcommand{\\argmin}{\\mathop{\\textrm{argmin}}}\n",
    "\\renewcommand{\\abs}[1]{\\left\\lvert#1\\right\\rvert}\n",
    "\\renewcommand{\\sign}{\\textrm{sign}}\n",
    "\\renewcommand{\\minimize}{\\mathop{\\textrm{minimize}}}\n",
    "\\renewcommand{\\subjectto}{\\mathop{\\textrm{subject to}}}\n",
    "\\renewcommand{\\evalat}[1]{\\Big|_{#1}} \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# (Deep) Neural networks compute through forward propagation\n",
    "Deep networks are built up out of layers.\n",
    "\n",
    "![](net.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# (Deep) Neural networks compute through forward propagation\n",
    "\n",
    "Let $f$ be an activation function and \n",
    "$$\n",
    "\\ff(\\zz)_i = f(z_i).\n",
    "$$\n",
    "\n",
    "We will denote number of variables in layer $l$ as $n_l$.\n",
    "\n",
    "Size of the zeroth layer, $n_0$, is number of features in a sample.\n",
    "\n",
    "\n",
    "With each layer, $l>0$, we assoiate a matrix of size $\\WW^l: n_l \\times n_{l-1}$ and a bias vector $\\bb^l : n_l$\n",
    "\n",
    "Recursive specification for forward propagation is given by\n",
    "$$\n",
    "\\hh^l = \\ff(\\WW^1\\hh^{l-1} + \\bb^l).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Forward propagation restated\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We restate forward propagation to make it easier to compute derivatives\n",
    "\n",
    "\\begin{align}\n",
    "\\hh^0 &= \\xx \\\\\n",
    "\\zz^l &= \\WW^l\\hh^{l-1} + \\bb^l \\\\\n",
    "\\hh^l &= \\ff(\\zz^l).\n",
    "\\end{align}\n",
    "![](forward-prop-detailed.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Common loss functions for Deep neural nets\n",
    "\n",
    "Output of the network is $\\hat{y}(\\xx) = \\hh^L$, where $L$ is the depth of the network.\n",
    "\n",
    "In order to asses how the network is performing we need an objective.\n",
    "$$\n",
    "E = \\sum_t \\textrm{loss}(\\hat{y}(\\xx^t),y^t)\n",
    "$$\n",
    "\n",
    "Loss for continuous $y$: \n",
    "$$\n",
    "\\textrm{loss}(\\hat{y},y) = \\frac{1}{2}(y-\\hat{y})^2\n",
    "$$\n",
    "\n",
    "Loss for a binary $y$: \n",
    "$$\n",
    "\\textrm{loss}(\\hat{y},y) = -y\\log(\\hat{y}) - (1 - y)\\log(1 - \\hat{y})\n",
    "$$\n",
    "\n",
    "**Q: We can work through a sanity check here. Is the loss larger when $y \\neq \\hat{y}$ than when $y = \\hat{y}$?** \n",
    "\n",
    "[Hint: You can assume that $0\\log0 = 0$]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Looking at the objective and forward propagation\n",
    "\n",
    "We have \n",
    "\n",
    "\\begin{align}\n",
    "\\hh^0 &= \\xx \\\\\n",
    "\\zz^l &= \\WW^l\\hh^{l-1} + \\bb^l && \\textrm{input into the $l$th layer activation } \\\\\n",
    "\\hh^l &= \\ff(\\zz^l) && \\textrm{output of the $l$th layer activation} \\\\\n",
    "\\hat{y}(\\xx) &= h^L && \\textrm{output is the last layer's state} \\\\\n",
    "\\end{align}\n",
    "\n",
    "The objective is given by\n",
    "$$\n",
    "E = \\sum_t \\textrm{loss}(\\hat{y}(\\xx),y^t).\n",
    "$$\n",
    "\n",
    "We are interested in minimizing the loss, so we need partial derviatives $$\\frac{\\partial E}{\\partial w^l_{ij}} \\textrm{ and } \\frac{\\partial E}{\\partial b^l_{i}}$$ for all layers $l=1,...L$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Frame of mind for back-prop(agation)\n",
    "\n",
    "Forward prop is a recursive procedure which propagates information upward to make a prediction.\n",
    "\n",
    "That error of that prediction $E$ can be reduced if we change parameter $w^l_{ij}$ according to the gradient \n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w^{l}_{ij}}\n",
    "$$\n",
    "Chain rule tels us that\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w^{l}_{ij}} = \\frac{\\partial E}{\\partial h^l_i}\\frac{\\partial h^l_i}{\\partial w^{l}_{ij}} \n",
    "$$\n",
    "\n",
    "![](dependencies.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Frame of mind for back-prop\n",
    "\n",
    "\n",
    "We compute how much of the error in the prediction is attributable to each parameter.\n",
    "\n",
    "To do this we will break down computation into two parts:\n",
    "1. computing how a hidden variable affects the objective -- across all orange paths\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial h^l_i}\n",
    "$$\n",
    "2. computing how parameter associated with the hidden variable affects the variable -- blue edge\n",
    "$$\n",
    "\\frac{\\partial h^l_i}{\\partial w^{l}_{ij}}\n",
    "$$\n",
    "\n",
    "![](dependencies.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Total derivative reminder \n",
    "\n",
    "\n",
    "Total derivative\n",
    "$$\n",
    "\\frac{\\partial\n",
    "f(\\zz(t)) }{\\partial t}= \\sum_i \\frac{\\partial f}{\\partial z_i}\\frac{\\partial z_i}{\\partial t} \n",
    "$$\n",
    "\n",
    "\n",
    "Let's say \n",
    "$$\n",
    "f(x,y,z) = x^2 + y^3 + z^4\n",
    "$$\n",
    "and let's say\n",
    "$$\n",
    "g(t) = f(t,t,1)\n",
    "$$\n",
    "then \n",
    "$$\n",
    "\\frac{\\partial g(t)}{\\partial t} = 2t + 3t^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluate-at notation\n",
    "Also, to be very explicit\n",
    "$$\n",
    "\\frac{\\partial f(\\zz)}{\\partial z_i}\\evalat{\\zz = \\zz_0}\n",
    "$$\n",
    "this is a partial derivative with respect to $z_i$ evaluated at $\\zz_0$.\n",
    "\n",
    "\n",
    "For example,\n",
    "$$\n",
    "\\frac{\\partial z_1^2 + z_2}{\\partial z_1}\\evalat{\\zz = [1.0,-1.0]} = 2z_1\\evalat{\\zz = [1.0,-1.0]} = 2\n",
    "$$\n",
    "\n",
    "Once we get used to this, I will drop the \"evaluate-at\" notation.\n",
    "\n",
    "As a rule of thumb, everything on the left of the vertical bar are variables and functions, **not** values.\n",
    "\n",
    "Values occur only on the right side of the bar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using chain rule to compute derivatives -- back propagation\n",
    "\n",
    "For completness, we are briefly going to be very explicit about the states of the neural network for different samples.\n",
    "\n",
    "Forward propagation on each of the samples gives us \n",
    "\n",
    "\\begin{align}\n",
    "\\hh^{0,t} &= \\xx^t  && \\textrm{sample t, feature vector}\\\\\n",
    "\\zz^{l,t} &= \\WW^l\\hh^{l-1,t} + \\bb^{l} && \\textrm{sample t, input $l$th layer activation}\\\\\n",
    "\\hh^{l,t} &= \\ff(\\zz^{l,t}) && \\textrm{sample t, output $l$th layer activation}\\\\\n",
    "\\hat{y}(\\xx^t) &= h^{L,t} && \\textrm{sample t, network prediction} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using chain rule to compute derivatives -- back propagation\n",
    "\n",
    "The objective is given by\n",
    "$$\n",
    "E = \\sum_t \\textrm{loss}(\\hat{y}(\\xx^t),y^t).\n",
    "$$\n",
    "\n",
    "Partial derivative\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial w^l_{ij} } &= \n",
    " \\sum_t\\frac{\\partial  \\textrm{loss}(\\hat{y},y^t)}{\\partial \\hat{y}}\\evalat{\\hat{y} = \\hat{y}(\\xx^t)}\\frac{ \\hat{y}}{\\partial h^L }\\evalat{h^L = h^{L,t}}\\frac{ \\partial h^L }{\\partial w^l_{ij}}\n",
    " \\end{align}\n",
    "\n",
    "Pausing here for a moment: $h^L$ is a variable, $h^{L,t}$ is a value computed through forward propagation starting with input $\\xx^t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simplifying math\n",
    "\n",
    "Carrying the sample index, $t$, and stating where the partial derivatives are evaluated is cumbersome.\n",
    "\n",
    "From this point on, we will compute partial derivative for a single sample and some parameter $\\theta$.\n",
    "\n",
    "Further, all derivatives will be evaluated at values obtained through forward propagation ($\\hh^l,\\zz^l$).\n",
    "\n",
    "Hence, this term\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\textrm{loss}(\\hat{y}(\\xx^t),y^t)}{\\partial \\theta} &= \\frac{\\partial  \\textrm{loss}(\\hat{y},y^t)}{\\partial \\hat{y}}\\evalat{\\hat{y} = \\hat{y}(\\xx^t)}\\frac{ \\hat{y}}{\\partial h^L }\\evalat{h^L = h^{L,t}}\\frac{ \\partial h^L }{\\partial \\theta}\\end{align}\n",
    "after dropping sample index and removing evalate-at notation, simplifies to\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\textrm{loss}(\\hat{y},y)}{\\partial \\theta} &= \n",
    "\\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{\\partial \\hat{y}}\\frac{ \\hat{y}}{\\partial h^L }\\frac{ \\partial h^L }{\\partial \\theta}\\end{align}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using chain rule to compute derivatives -- back propagation\n",
    "\n",
    "\n",
    "\n",
    "We are going to use the facts that $\\hh^m$ depends on $\\zz^{m}$ and $\\zz^m$ depends on $\\hh^{m-1}$ to keep expanding the derivative to highlight a pattern\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{ \\partial \\theta} &= \\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{\\partial \\hat{y}}\\sum_{i=1}^{n_L}\\frac{ \\hat{y}}{\\partial h_i^L } \\frac{ \\partial h_i^L}{\\partial z_i^L}\\frac{ \\partial z_i^L}{\\partial \\theta}\\\\\n",
    "&= \\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{\\partial \\hat{y}}\\sum_{i=1}^{n_L}\\frac{ \\hat{y}}{\\partial h_i^L } \\frac{ \\partial h^L}{\\partial z_i^L}\\sum_{j=1}^{n_{L-1}} \\frac{ \\partial z_i^L}{\\partial h_j^{L-1}}\\frac{ \\partial h_j^{L-1}}{\\partial \\theta}\\\\\n",
    "&=\\sum_{j=1}^{n_{L-1}}\\underbrace{\\sum_{i=1}^{n_L} \\overbrace{\\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{\\partial \\hat{y}}\\frac{ \\hat{y}}{\\partial h_i^L }}^{\\partial  \\textrm{loss}(\\hat{y},y)/\\partial h_i^{L}} \\frac{ \\partial h_i^L}{\\partial z_i^L} \\frac{ \\partial z_i^L}{\\partial h_j^{L-1}}}_{\\partial  \\textrm{loss}(\\hat{y},y)/\\partial h_j^{L-1}} \\frac{ \\partial h_j^{L-1}}{\\partial  \\theta}\\\\\n",
    "\\end{align}\n",
    "\n",
    "Hence, we can see a recursion needed to compute $\\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{\\partial h_j^{L-1}}$ using $\\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{\\partial h_i^{L}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using chain rule to compute derivatives -- back propagation\n",
    "\n",
    "\n",
    "Start of recursion at $L$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{\\partial h_i^{L}} &= \\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{\\partial \\hat{y}}\\frac{ \\hat{y}}{\\partial h_i^L }\n",
    "\\end{align}\n",
    "\n",
    "Recursive rule, going **back** from layer $l$ to $l-1$:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{\\partial h_j^{l-1}} &= \\sum_{i=1}^{n_{l}}\\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{\\partial h_i^{l}} \\frac{\\partial h^l_i}{\\partial z^l_i}\\frac{\\partial z^l_i}{\\partial h_j^{l-1}}\\\\\n",
    "&= \\sum_{i=1}^{n_{l}}\\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{\\partial h_i^{l}} f'(z^l_i)w^l_{ij}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap\n",
    "\n",
    "Given a neural network we know how to use **forward propagation** to obtain $\\hh$ and $\\zz$.\n",
    "\n",
    "\n",
    "Using these $\\hh$ and $\\zz$ we can use a **back propagation** to compute $\\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{\\partial h_i^{l}}$\n",
    "\n",
    "We can plug-in computed $\\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{\\partial h_i^{l}}$ to obtain derivatives of the loss with respect to parameters \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\theta } &= \n",
    "\\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{\\partial h_i^{l}}\\frac{ \\partial h_i^L }{\\partial \\theta}\n",
    "\\end{align}\n",
    "\n",
    "where $\\theta$ is either $w^l_{ij}$ or $b^l_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Computing derivatives within a layer\n",
    "<br>\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\theta } &= \n",
    "\\underbrace{\\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{\\partial h_i^{l}}}_{\\textrm{know how to compute}}\\underbrace{\\frac{ \\partial h_i^L }{\\partial \\theta}}_{\\textrm{need to figure out}}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Hence\n",
    "$$\n",
    "\\frac{ \\partial h_i^L }{\\partial \\theta}  = \\frac{ \\partial h_i^L }{\\partial z_i^l}\\frac{ \\partial z_i^L }{\\partial \\theta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "$$\n",
    "\\frac{ \\partial h_i^L }{\\partial \\theta}  = \\frac{ \\partial h_i^L }{\\partial z_i^l}\\frac{ \\partial z_i^L }{\\partial \\theta}\n",
    "$$\n",
    "Gives us\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial h_i^l}{\\partial w^l_{ij}} &=   f'(z^l_i)h^{l-1}_j \\\\ \n",
    "\\frac{\\partial h_i^l}{\\partial b^l_{i}} &=  f'(z^l_i)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Back-propagation: putting it together\n",
    "1. Run forward prop\n",
    "\\begin{align}\n",
    "\\hh^0 &= \\xx \\\\\n",
    "\\zz^l &= \\WW^l\\hh^{l-1} + \\bb^l && \\textrm{input into the $l$th layer activation } \\\\\n",
    "\\hh^l &= \\ff(\\zz^l) && \\textrm{output of the $l$th layer activation} \\\\\n",
    "\\hat{y}(\\xx) &= h^L && \\textrm{output is the last layer's state} \\\\\n",
    "\\end{align}\n",
    "2. Run backward prop\n",
    "\\begin{align}\n",
    "\\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{\\partial h_i^{L}} &= \\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{\\partial \\hat{y}}\\frac{ \\hat{y}}{\\partial h_i^L }\\\\\n",
    "\\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{\\partial h_j^{l-1}} &= \\sum_{i=1}^{n_{l}}\\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{\\partial h_i^{l}} f'(z^l_i)w^l_{ij}\n",
    "\\end{align}\n",
    "3. Compute gradients\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{w^l_{ij} } &= \n",
    "\\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{\\partial h_i^{l}} f'(z^l_i)h^{l-1}_j\\\\\n",
    "\\frac{\\partial E}{b^l_{i} } &= \\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{\\partial h_i^{l}} f'(z^l_i)\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loss(yhat,y):\n",
    "    # loss(yhat,y)\n",
    "    l = 0.5*(yhat - y)**2.0\n",
    "    # d loss(yhat,y) / d yhat\n",
    "    g = (yhat - y)\n",
    "    return l,g\n",
    "\n",
    "def sigmoid(z):    \n",
    "    # if z is a vector\n",
    "    # h is a vector of outputs\n",
    "    # g is derivative of f at each z\n",
    "    h = 1.0/(1.0 + np.exp(-z))\n",
    "    g = h*(1.0 - h)  \n",
    "    return h,g\n",
    "\n",
    "def compute_loss(loss,predict,x,y):\n",
    "    yhat = predict(x)\n",
    "    val,_ = loss(yhat,y)\n",
    "    return val.squeeze()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "![](xor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. Run forward prop\n",
    "\\begin{align}\n",
    "\\hh^0 &= \\xx \\\\\n",
    "\\zz^l &= \\WW^l\\hh^{l-1} + \\bb^l && \\textrm{input into the $l$th layer activation } \\\\\n",
    "\\hh^l &= \\ff(\\zz^l) && \\textrm{output of the $l$th layer activation} \\\\\n",
    "\\hat{y}(\\xx) &= h^L && \\textrm{output is the last layer's state} \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  [[ 0.  0.]] Output:  0.149132886381\n",
      "Input:  [[ 0.  1.]] Output:  0.893163783089\n",
      "Input:  [[ 1.  0.]] Output:  0.893163783089\n",
      "Input:  [[ 1.  1.]] Output:  0.149132886381\n"
     ]
    }
   ],
   "source": [
    "def forward_prop(x,f,Ws,bs):\n",
    "    # simple implementation of forward prop    \n",
    "    hs = [x]               # store h^0\n",
    "    zs = [np.zeros(len(x))] # store z^0\n",
    "    h = x                  # current layer is 0 \n",
    "    L = len(Ws)            \n",
    "    for l in range(L):     \n",
    "        Wl = Ws[l]            # get layer weights\n",
    "        bl = bs[l]            # get layer biases\n",
    "        nl = Wl.shape[0]      # number of variables in \n",
    "                              # layer l        \n",
    "        z = np.dot(Wl,h) + bl # input into layer l\n",
    "        h,_ = f(z)            # activation for each z\n",
    "        zs.append(z)        \n",
    "        hs.append(h)\n",
    "    return hs,zs\n",
    "\n",
    "# XOR net \n",
    "Ws = []\n",
    "bs = []\n",
    "F = 5.0\n",
    "# first layer \n",
    "Ws.append(F*np.asarray([[1.0,-1.0],\n",
    "                        [-1.0,1.0]]))\n",
    "bs.append(F*np.asarray([[-0.5],\n",
    "                      [-0.5]]))\n",
    "# second layer weights\n",
    "Ws.append(F*np.asarray([[1.0,1.0]]))\n",
    "bs.append(F*np.asarray([[-0.5]]))\n",
    "\n",
    "for x1 in [0.0,1.0]:\n",
    "    for x2 in [0.0,1.0]:\n",
    "        x = np.asarray([[x1],[x2]])\n",
    "        hs,zs = forward_prop(x,sigmoid,Ws,bs)\n",
    "        print \"Input: \",x.T,\"Output: \",hs[-1].squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2. Run backward prop\n",
    "\\begin{align}\n",
    "\\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{\\partial h_i^{L}} &= \\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{\\partial \\hat{y}}\\frac{ \\hat{y}}{\\partial h_i^L }\\\\\n",
    "\\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{\\partial h_j^{l-1}} &= \\sum_{i=1}^{n_{l}}\\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{\\partial h_i^{l}} f'(z^l_i)w^l_{ij}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def backward_prop(loss,f,Ws,bs,zs,y):\n",
    "    _,dldh = loss(hs[-1],y)\n",
    "    L = len(Ws)\n",
    "    dldhs = [[]]*(L+1) # one per layer    \n",
    "    dldhs[L] = dldh  # derivative of loss with respect to\n",
    "                     # prediction (yhat)\n",
    "    for l in reversed(range(L)):\n",
    "        Wl = Ws[l]\n",
    "        bl = bs[l]\n",
    "        nl = Wl.shape[0]\n",
    "        n = Wl.shape[1]\n",
    "        dldhs[l] = np.zeros((n,1))\n",
    "        for i in range(nl):\n",
    "            _,df = f(zs[l+1][i]) \n",
    "            m = dldhs[l+1][i]*df\n",
    "            for j in range(n):                            \n",
    "                dldhs[l][j] = m*Wl[i,j]\n",
    "    return dldhs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3. Compute gradients\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{w^l_{ij} } &= \n",
    "\\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{\\partial h_i^{l}} f'(z^l_i)h^{l-1}_j\\\\\n",
    "\\frac{\\partial E}{b^l_{i} } &= \\frac{\\partial  \\textrm{loss}(\\hat{y},y)}{\\partial h_i^{l}} f'(z^l_i)\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def compute_gradients(dldhs,f,zs,hs):\n",
    "    L = len(dldhs)-1\n",
    "    dWs = [[]]*L\n",
    "    dbs = [[]]*L\n",
    "    for l in range(L):\n",
    "        nl = len(hs[l+1])\n",
    "        n = len(hs[l])\n",
    "        dWs[l] = np.zeros((nl,n))\n",
    "        for i in range(nl):             \n",
    "            _,g = f(zs[l+1][i])\n",
    "            hl = hs[l].squeeze()\n",
    "            dWs[l][i,:] = dldhs[l+1][i]*g*hl\n",
    "            dbs[l] = dldhs[l+1][i]*g\n",
    "            \n",
    "            \n",
    "    return dWs,dbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0031136092315531272, 0.0031136087408337918)\n",
      "(0.0031136092315531272, 0.0031136087408337918)\n",
      "(-0.0047451058360348242, -0.0047451063432885328)\n",
      "(-0.0047451058360348242, -0.0047451063432885328)\n",
      "(0.05932591112811636, 0.059325910360931711)\n",
      "(0.053831228363776518, 0.053831228615190692)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Gradient Checking code using finite diffrences\n",
    "# \n",
    "x = np.asarray([[1.0],[1.0]])\n",
    "y = np.asarray([[0.0]])\n",
    "Ws[0] = 0.1*np.random.randn(2,2)\n",
    "Ws[1] = 0.1*np.random.randn(1,2)\n",
    "bs[0] = 0.1*np.random.randn(2,1)\n",
    "bs[1] = 0.1*np.random.randn(1,1)\n",
    "hs,zs = forward_prop(x,sigmoid,Ws,bs)\n",
    "dldhs = backward_prop(loss,sigmoid,Ws,bs,zs,y)  \n",
    "dWs,dbs = compute_gradients(dldhs,sigmoid,zs,hs)\n",
    "\n",
    "L = len(dWs)\n",
    "eps = 1e-7\n",
    "fddWs = [[]]*L\n",
    "\n",
    "\n",
    "for l in range(L):\n",
    "    fddWs[l] = np.zeros(Ws[l].shape)\n",
    "    for i in range(Ws[l].shape[0]):\n",
    "        for j in range(Ws[l].shape[1]):\n",
    "            Wc = Ws[l].copy()\n",
    "            Ws[l][i,j] = Wc[i,j] + 0.5*eps            \n",
    "            predict = lambda x: forward_prop(x,sigmoid,Ws,bs)[0][-1].squeeze()\n",
    "            v1 = compute_loss(loss,predict,x,y)\n",
    "            Ws[l][i,j] = Wc[i,j] - 0.5*eps            \n",
    "            predict = lambda x: forward_prop(x,sigmoid,Ws,bs)[0][-1].squeeze()\n",
    "            v2 = compute_loss(loss,predict,x,y)\n",
    "            Ws[l] = Wc\n",
    "            fddWs[l][i,j] = (v1 - v2)/eps\n",
    "            print(fddWs[l][i,j],dWs[l][i,j])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.random.seed(1)\n",
    "Ws[0] = 0.5*np.random.randn(2,2)\n",
    "Ws[1] = 0.5*np.random.randn(1,2)\n",
    "bs[0] = 0.5*np.random.randn(2,1)\n",
    "bs[1] = 0.5*np.random.randn(1,1)\n",
    "\n",
    "\n",
    "N = 1000\n",
    "x = (np.random.rand(2,N)>0.5).astype('float')\n",
    "x = x + 0.1*np.random.randn(2,N)\n",
    "\n",
    "y = np.logical_xor(x[0,:]>0.5,x[1,:]>0.5).astype('float')\n",
    "\n",
    "L = 2\n",
    "eta = 3e-2\n",
    "Es = []\n",
    "for it in range(200):\n",
    "    E = 0    \n",
    "    for l in range(L):\n",
    "        dWs[l] = np.zeros(Ws[l].shape)\n",
    "        dbs[l] = np.zeros(bs[l].shape)\n",
    "        \n",
    "    for t in range(N):\n",
    "        xt = x[:,[t]]\n",
    "        yt = y[t]\n",
    "        hs,zs = forward_prop(xt,sigmoid,Ws,bs)\n",
    "        dldhs = backward_prop(loss,sigmoid,Ws,bs,zs,yt)  \n",
    "        dtWs,dtbs = compute_gradients(dldhs,sigmoid,zs,hs)\n",
    "        for l in range(L):\n",
    "            dWs[l] = dWs[l] + dtWs[l]\n",
    "            dbs[l] = dbs[l] + dtbs[l]            \n",
    "        Et = loss(hs[-1][0][0],yt)[0]\n",
    "        E = E + Et    \n",
    "    for l in range(L):\n",
    "        Ws[l] = Ws[l] - eta*dWs[l]\n",
    "        bs[l] = bs[l] - eta*dbs[l]        \n",
    "    Es.append(E)     \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First layer weights:\n",
      "[[-3.2933533  -3.3736506 ]\n",
      " [-6.57363912 -6.7182171 ]]\n",
      "First layer biases:\n",
      "[[ 4.72145779]\n",
      " [ 3.46844846]]\n",
      "Second layer weights:\n",
      "[[ 9.89952122 -9.51866265]]\n",
      "Second layer biases:\n",
      "[[-4.20933528]]\n",
      "Input:  [[ 0.  0.]] Output:  0.0258780857113\n",
      "Input:  [[ 0.  1.]] Output:  0.96419601442\n",
      "Input:  [[ 1.  0.]] Output:  0.966669564212\n",
      "Input:  [[ 1.  1.]] Output:  0.0487062060114\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAicAAAF5CAYAAABEPIrHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7eea748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Es)\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('E')\n",
    "print \"First layer weights:\\n\",Ws[0]\n",
    "print \"First layer biases:\\n\",bs[0]\n",
    "print \"Second layer weights:\\n\",Ws[1]\n",
    "print \"Second layer biases:\\n\",bs[1]\n",
    "\n",
    "for x1 in [0.0,1.0]:\n",
    "    for x2 in [0.0,1.0]:\n",
    "        x = np.asarray([[x1],[x2]])\n",
    "        hs,zs = forward_prop(x,sigmoid,Ws,bs)\n",
    "        print \"Input: \",x.T,\"Output: \",hs[-1].squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap\n",
    "\n",
    "We introduced back-propagation.\n",
    "\n",
    "It decomposed computation of partial derivatives of the loss wrt parameters into two tasks\n",
    "1. derivative of loss with respect to hidden variables\n",
    "2. derivative of the hidden variable with respect to parameter\n",
    "\n",
    "First task was accomplished using chain rule to yield a recursive formula which works backwards from the loss -- back-propagation.\n",
    "\n",
    "Second task was accomplished by taking derivatives within a layer.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nbpresent": {
   "slides": {
    "0351a772-c92a-4430-84b4-7130549e7ab0": {
     "id": "0351a772-c92a-4430-84b4-7130549e7ab0",
     "prev": "1ae7150b-6121-40bd-8f33-b1b0690791e2",
     "regions": {
      "da29fdf0-46b3-49f8-8047-2d5d7254a245": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "fbcdc5fe-8e7e-4180-9764-ecad0ef709ea",
        "part": "whole"
       },
       "id": "da29fdf0-46b3-49f8-8047-2d5d7254a245"
      }
     }
    },
    "12b628e8-f03d-4ade-acba-73b1c75754a9": {
     "id": "12b628e8-f03d-4ade-acba-73b1c75754a9",
     "prev": "ffc1d54c-925e-4648-8d83-24541d52be7c",
     "regions": {
      "7d32489a-204e-4b5c-b95b-a9fd5f7e447a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "b934daf2-1dfd-4706-a216-f46f107d3fb7",
        "part": "whole"
       },
       "id": "7d32489a-204e-4b5c-b95b-a9fd5f7e447a"
      }
     }
    },
    "14c534af-5d17-41c3-a3b0-81839aa8692a": {
     "id": "14c534af-5d17-41c3-a3b0-81839aa8692a",
     "prev": "a7c7f5ef-322e-4638-9a05-4e6aab630bd7",
     "regions": {
      "f7032b61-9587-4bf6-99ef-c455b56a349c": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "8c34e974-39aa-46da-a01d-8cdfc4ee35c2",
        "part": "whole"
       },
       "id": "f7032b61-9587-4bf6-99ef-c455b56a349c"
      }
     }
    },
    "1ae7150b-6121-40bd-8f33-b1b0690791e2": {
     "id": "1ae7150b-6121-40bd-8f33-b1b0690791e2",
     "prev": "8600b916-dfb8-44a1-876a-6d80797c5899",
     "regions": {
      "d22b7fc2-48b8-4413-ad89-eb0b182d455a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e30a931b-932b-4b01-b4c1-884208eb4530",
        "part": "whole"
       },
       "id": "d22b7fc2-48b8-4413-ad89-eb0b182d455a"
      }
     }
    },
    "292fd08b-58d1-4a30-ae20-3a0ab5320860": {
     "id": "292fd08b-58d1-4a30-ae20-3a0ab5320860",
     "prev": "ddcfa80f-2c65-4afe-aa32-296ec97b2344",
     "regions": {
      "cbe3fa3e-c517-4207-ac11-d884ffa1f3ed": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "7cbecfc4-fa24-414c-82e4-fbb242fa3faf",
        "part": "whole"
       },
       "id": "cbe3fa3e-c517-4207-ac11-d884ffa1f3ed"
      }
     }
    },
    "2cdbba23-1148-4f6c-8549-692283c5e35c": {
     "id": "2cdbba23-1148-4f6c-8549-692283c5e35c",
     "prev": "eff734ad-96ec-463a-87fb-dda40cc8ce48",
     "regions": {
      "9724d964-f1ea-4f62-a7da-9bf35dfad5e7": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e7cc0d43-4957-4271-bf7b-0a35ae71117c",
        "part": "whole"
       },
       "id": "9724d964-f1ea-4f62-a7da-9bf35dfad5e7"
      }
     }
    },
    "401fafc4-bb94-4bc6-a8b2-dee06ae8ae8b": {
     "id": "401fafc4-bb94-4bc6-a8b2-dee06ae8ae8b",
     "prev": "2cdbba23-1148-4f6c-8549-692283c5e35c",
     "regions": {
      "5b755dea-45ce-44d7-8dce-aecbebaabd58": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "107f8412-5f74-449a-b934-156820f0bbcf",
        "part": "whole"
       },
       "id": "5b755dea-45ce-44d7-8dce-aecbebaabd58"
      }
     }
    },
    "5446f7ea-d6d7-47b1-bba5-5bcec2a4fe25": {
     "id": "5446f7ea-d6d7-47b1-bba5-5bcec2a4fe25",
     "prev": "c5b602e5-22d8-4689-8a4c-acb935322846",
     "regions": {
      "df60a52d-fc25-4b64-b6c9-a04507dc6d76": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "14bb883c-5a7f-4e4e-b0ff-ad59c985dcb2",
        "part": "whole"
       },
       "id": "df60a52d-fc25-4b64-b6c9-a04507dc6d76"
      }
     }
    },
    "5623e543-fe11-4ac1-9102-c01ba6f47b2d": {
     "id": "5623e543-fe11-4ac1-9102-c01ba6f47b2d",
     "prev": null,
     "regions": {
      "2ecffa8e-1c79-43ef-b42d-bffa7cc4e3e7": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "aab7fd3c-11a1-4073-a957-c10735d7afae",
        "part": "whole"
       },
       "id": "2ecffa8e-1c79-43ef-b42d-bffa7cc4e3e7"
      }
     }
    },
    "5b0c87d2-037c-4ed2-bed3-f490bfa70ccf": {
     "id": "5b0c87d2-037c-4ed2-bed3-f490bfa70ccf",
     "prev": "6c826828-2be0-4c2e-b58f-af16d6eb14ad",
     "regions": {
      "02e13c85-fd99-4af5-a680-3d217e01cc92": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "84557d87-6b9c-4ddb-a44c-aac2c6b74479",
        "part": "whole"
       },
       "id": "02e13c85-fd99-4af5-a680-3d217e01cc92"
      }
     }
    },
    "6c826828-2be0-4c2e-b58f-af16d6eb14ad": {
     "id": "6c826828-2be0-4c2e-b58f-af16d6eb14ad",
     "prev": "14c534af-5d17-41c3-a3b0-81839aa8692a",
     "regions": {
      "432a5f24-6769-4f43-972d-ea82d6922c05": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "35539823-9e9b-4458-b888-23b67b4d6553",
        "part": "whole"
       },
       "id": "432a5f24-6769-4f43-972d-ea82d6922c05"
      }
     }
    },
    "8600b916-dfb8-44a1-876a-6d80797c5899": {
     "id": "8600b916-dfb8-44a1-876a-6d80797c5899",
     "prev": "865414d8-8321-4fbc-b7d3-fce492038da5",
     "regions": {
      "5855e7e6-e804-4aa5-8cf9-4479a558a6e7": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "4da9ff24-f0fe-4719-8e6a-275de188652f",
        "part": "whole"
       },
       "id": "5855e7e6-e804-4aa5-8cf9-4479a558a6e7"
      }
     }
    },
    "865414d8-8321-4fbc-b7d3-fce492038da5": {
     "id": "865414d8-8321-4fbc-b7d3-fce492038da5",
     "prev": "9b359c37-fe53-4ab5-a4f8-8b72091c2fab",
     "regions": {
      "62f2b0b2-bf80-4524-bb71-17cc3da900cb": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "7d3a96df-84a2-481a-a4e4-49ac4837dfbf",
        "part": "whole"
       },
       "id": "62f2b0b2-bf80-4524-bb71-17cc3da900cb"
      }
     }
    },
    "9b359c37-fe53-4ab5-a4f8-8b72091c2fab": {
     "id": "9b359c37-fe53-4ab5-a4f8-8b72091c2fab",
     "prev": "5623e543-fe11-4ac1-9102-c01ba6f47b2d",
     "regions": {
      "7bcfde74-125c-4cea-afdb-a2c39805f36c": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e2b9a449-7f63-483e-9e3d-734d6c16887a",
        "part": "whole"
       },
       "id": "7bcfde74-125c-4cea-afdb-a2c39805f36c"
      }
     }
    },
    "a7c7f5ef-322e-4638-9a05-4e6aab630bd7": {
     "id": "a7c7f5ef-322e-4638-9a05-4e6aab630bd7",
     "prev": "401fafc4-bb94-4bc6-a8b2-dee06ae8ae8b",
     "regions": {
      "6dd5826b-c385-41b7-b84a-2995f50f89a6": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "595413d3-19fa-4bee-9f22-ced173614597",
        "part": "whole"
       },
       "id": "6dd5826b-c385-41b7-b84a-2995f50f89a6"
      }
     }
    },
    "c5b602e5-22d8-4689-8a4c-acb935322846": {
     "id": "c5b602e5-22d8-4689-8a4c-acb935322846",
     "prev": "12b628e8-f03d-4ade-acba-73b1c75754a9",
     "regions": {
      "609ea961-e72b-4ed6-8041-ee84c2a9203f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "0d29e769-fc04-4a74-9eb2-30e115210e9c",
        "part": "whole"
       },
       "id": "609ea961-e72b-4ed6-8041-ee84c2a9203f"
      }
     }
    },
    "d4238c0e-8429-40b5-90fd-4e82eafd70b5": {
     "id": "d4238c0e-8429-40b5-90fd-4e82eafd70b5",
     "prev": "5b0c87d2-037c-4ed2-bed3-f490bfa70ccf",
     "regions": {
      "07e14a93-6398-42fe-b609-7caea3857a56": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "f60716a0-d1a3-4852-ae64-94199ef71bbe",
        "part": "whole"
       },
       "id": "07e14a93-6398-42fe-b609-7caea3857a56"
      }
     }
    },
    "ddcfa80f-2c65-4afe-aa32-296ec97b2344": {
     "id": "ddcfa80f-2c65-4afe-aa32-296ec97b2344",
     "prev": "5446f7ea-d6d7-47b1-bba5-5bcec2a4fe25",
     "regions": {
      "f296dad0-361a-4660-a095-e82b3adf102a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "7d9ddcb3-65ab-4635-95f1-f919c084cf8a",
        "part": "whole"
       },
       "id": "f296dad0-361a-4660-a095-e82b3adf102a"
      }
     }
    },
    "eff734ad-96ec-463a-87fb-dda40cc8ce48": {
     "id": "eff734ad-96ec-463a-87fb-dda40cc8ce48",
     "prev": "292fd08b-58d1-4a30-ae20-3a0ab5320860",
     "regions": {
      "3e138a53-a7b2-44d0-bff0-379033d9849d": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "cb0cc60b-b4e5-400d-9e36-6370185f40b1",
        "part": "whole"
       },
       "id": "3e138a53-a7b2-44d0-bff0-379033d9849d"
      }
     }
    },
    "ffc1d54c-925e-4648-8d83-24541d52be7c": {
     "id": "ffc1d54c-925e-4648-8d83-24541d52be7c",
     "prev": "0351a772-c92a-4430-84b4-7130549e7ab0",
     "regions": {
      "251cd137-0738-4f41-b40d-1498d0ff49e6": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "1b3011bf-b293-454a-b391-c366d739f399",
        "part": "whole"
       },
       "id": "251cd137-0738-4f41-b40d-1498d0ff49e6"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
