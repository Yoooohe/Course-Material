{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# COMP 755\n",
    "\n",
    "Plan for today\n",
    "\n",
    "1. Review Generative models for classification\n",
    "2. Naive Bayes with Bernoulli feature distribution\n",
    "3. Tuning and evaluating models\n",
    "    * Cross-Validation\n",
    "    * ROC plots\n",
    "    \n",
    "\n",
    "$$\n",
    "\\renewcommand{\\xx}{\\mathbf{x}}\n",
    "\\renewcommand{\\yy}{\\mathbf{y}}\n",
    "\\renewcommand{\\zz}{\\mathbf{z}}\n",
    "\\renewcommand{\\vv}{\\mathbf{v}}\n",
    "\\renewcommand{\\bbeta}{\\boldsymbol{\\mathbf{\\beta}}}\n",
    "\\renewcommand{\\mmu}{\\boldsymbol{\\mathbf{\\mu}}}\n",
    "\\renewcommand{\\ssigma}{\\boldsymbol{\\mathbf{\\sigma}}}\n",
    "\\renewcommand{\\reals}{\\mathbb{R}}\n",
    "\\renewcommand{\\loglik}{\\mathcal{LL}}\n",
    "\\renewcommand{\\penloglik}{\\mathcal{PLL}}\n",
    "\\renewcommand{\\likelihood}{\\mathcal{L}}\n",
    "\\renewcommand{\\Data}{\\textrm{Data}}\n",
    "\\renewcommand{\\given}{ \\big| }\n",
    "\\renewcommand{\\MLE}{\\textrm{MLE}}\n",
    "\\renewcommand{\\tth}{\\textrm{th}}\n",
    "\\renewcommand{\\Gaussian}[2]{\\mathcal{N}\\left(#1,#2\\right)}\n",
    "\\renewcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\renewcommand{\\ones}{\\mathbf{1}}\n",
    "\\renewcommand{\\diag}[1]{\\textrm{diag}\\left( #1 \\right)}\n",
    "\\renewcommand{\\sigmoid}[1]{\\sigma\\left(#1\\right)}\n",
    "\\renewcommand{\\myexp}[1]{\\exp\\left\\{#1\\right\\}}\n",
    "\\renewcommand{\\mylog}[1]{\\log\\left\\{#1\\right\\}}\n",
    "\\renewcommand{\\argmax}{\\mathop{\\textrm{argmax}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multivariate Gaussian distribution -- dependent case\n",
    "\n",
    "Suppose we have $n$ standard random variables  (0 mean, unit variance)\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z_i \\sim& \\Gaussian{0}{1},&  i=1,\\dots n\n",
    "\\end{aligned}\n",
    "$$\n",
    "and we are given a vector $\\mmu$ of length $n$ and a full-rank matrix $A$ of size $n \\times n$.\n",
    "\n",
    "Distribution of $\\xx = A\\zz + \\mu$ is\n",
    "$$\n",
    "p(\\xx) = \\left(2\\pi\\right)^{-\\frac{n}{2}}(\\det{\\Sigma})^{-\\frac{1}{2}}\\myexp{-\\frac{1}{2}(\\xx - \\mmu)^T\\Sigma^{-1}(\\xx-\\mmu)}\n",
    "$$\n",
    "where $\\Sigma = AA^T$.\n",
    "\n",
    "* $\\mu$ is **mean** of the Gaussian\n",
    "* $\\Sigma$ is **covariance** matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Maximum likelihood estimates of mean and covariance\n",
    "\n",
    "Given data $\\{\\xx_i \\in \\reals^n|i=1,\\dots,T\\}$ maximum likelihood estimates (MLE) of mean and covariance are:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mmu^{\\MLE} &= \\frac{1}{T}\\sum_{i=1}^T \\xx_i\\\\\n",
    "\\Sigma^{\\MLE} & = \\frac{1}{T}\\sum_{i=1}^T \\underbrace{\\left(\\xx_i - \\mmu^{\\MLE}\\right)\\left(\\xx_i - \\mmu^{\\MLE}\\right)^T}_{\\textrm{ a matrix of size $n \\times n$}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Dimensionality\n",
    "* $\\mmu^{\\MLE}$ is of same dimension as a single data point $n \\times 1$.\n",
    "* $\\Sigma^{\\MLE}$ is a matrix of size $n \\times n$ \n",
    "\n",
    "Note that $\\xx\\xx^T$ and $\\xx^T\\xx$ are not the same. Former is a matrix, latter is a scalar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generative models for classification \n",
    "\n",
    "There are two ways to factorize joint probability of labels and features\n",
    "$$\n",
    "p(y,\\xx\\given\\theta) = p(y\\given\\xx,\\theta)p(\\xx\\given\\theta) =  p(\\xx\\given y,\\theta)p(y\\given\\theta) \n",
    "$$\n",
    "\n",
    "The second one given us a simple process to *GENERATE* data:\n",
    "\n",
    "1. First select label according $p(y\\given\\theta)$, say it was $c$\n",
    "2. Now generate features $p(\\xx\\given y=c,\\theta)$\n",
    "\n",
    "Once we have such a model we can obtain the conditional probability $p(y\\given\\xx)$ using Bayes rule\n",
    "$$\n",
    "p(y=c\\given \\xx) = \\frac{p(y=c\\given\\theta)p(\\xx\\given y=c,\\theta)}{\\sum_k p(y=k\\given\\theta)p(\\xx\\given y=k,\\theta)}\n",
    "$$\n",
    "and we can predict label for a new feature vector $\\xx$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Naive Bayes\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(y = c \\given \\pi ) &= \\pi_k \\\\\n",
    "p(\\xx \\given y=c, \\theta) &= \\prod_j p(x_j \\given y= c,\\theta_{j,c})\n",
    "\\end{aligned}\n",
    "$$\n",
    "Parameters are \n",
    "* $\\pi_c$ prior probability that a sample comes from the class $c$\n",
    "* $\\theta_{j,c}$ parameters for the $j^\\tth$ feature for class $c$\n",
    "\n",
    "In general, there are many variants of Naive Bayes. \n",
    "\n",
    "You can choose different distributions for $p(x_j \\given y = c)$\n",
    "* Gaussian -- continuous features\n",
    "* Bernoulli -- binary features\n",
    "* Binomial -- count of positive outcomes\n",
    "* Categorical -- discrete features\n",
    "* Multinomial -- count of particular discrete outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bag of Words representation\n",
    "\n",
    "Review 188_7:\n",
    "\"I'm a **huge** **classic** film **buff**, but am just **getting** in to **silent** **movies**. \n",
    "A **lot** of **silent** **films** don't **hold** my **attention**, ...\"\n",
    "\n",
    "Review 196_9:\n",
    "\"... **fans** of the **silent** **era**, with many **cameos**, **adds** to the **overall** **fun** ...\"\n",
    "\n",
    "Converted into a row of word counts\n",
    "\n",
    "|   Document_id   | #attention | ... | #classic | ... | #fun   | ... | #silent | ... |\n",
    "| ---------       | ---        | --- | ---      | --- | ---    | --- | ---     | --- |\n",
    "|    188_7        |  1         | ... | 1        | ... | 0      | ... | 2       | ... | \n",
    "|    196_9        |  0         | ... | 0        | ... | 1      | ... | 1       | ... |\n",
    "| ...             | ...        | ... |  ...     | ... | ...    | ... | ...     | ... |\n",
    "\n",
    "\n",
    "Features can also be word presence/absence, rather than counts as above. \n",
    "\n",
    "These types of representations are called **bag-of-words**. \n",
    "\n",
    "In your homework we use bag-of-words representation of movie reviews to predict sentiment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Naive Bayes for spam classification\n",
    "\n",
    "One approach to classifying e-mail spam (1) vs not spam (0) is to construct a Naive Bayes model using a bag-of-words representation.\n",
    "\n",
    "Feature vector $\\xx$ is $W$ long vector of word presence absence in an e-mail \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(y =1 ) &= \\pi  &&\\textrm{prior probability that message is spam} \\\\\n",
    "p(y = 0 ) &= 1-\\pi  &&\\textrm{prior probability that message is  not spam} \\\\\n",
    "p(x_{j}  = 1 \\given y=1) &= \\theta_{1,j} && \\textrm{ probability that word $j$ appears in a spam e-mail} \\\\\n",
    "p(x_{j}  = 0 \\given y=1) &= 1-\\theta_{1,j} && \\\\\n",
    "p(x_{j}  = 1 \\given y=0) &= \\theta_{0,j} && \\textrm{ probability that word $j$ appears in non-spam e-mail} \\\\\n",
    "p(x_{j}  = 0 \\given y=0) &= 1-\\theta_{0,j} && \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Q: What is the size of $\\pi$?\n",
    "\n",
    "Q: What is the size of $\\theta$?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes for spam classification \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(y =1 ) &= \\pi  &&\\textrm{prior probability that message is spam} \\\\\n",
    "p(y =0 ) &= 1-\\pi  &&\\textrm{prior probability that message is  not spam} \\\\\n",
    "p(x_{j}  = 1 \\given y=1) &= \\theta_{1,j} && \\textrm{ probability that word $j$ appears in a spam e-mail} \\\\\n",
    "p(x_{j}  = 0 \\given y=1) &= 1-\\theta_{1,j} && \\textrm{ probability that word $j$ is absent in a spam e-mail}\\\\\n",
    "p(x_{j}  = 1 \\given y=0) &= \\theta_{0,j} && \\textrm{ probability that word $j$ is absent in non-spam e-mail} \\\\\n",
    "p(x_{j}  = 0 \\given y=0) &= 1-\\theta_{0,j} && \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "More compactly for math purposes\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(y) &= \\pi^y(1-\\pi)^{1-y}\\\\\n",
    "p(x_{j}\\given y) &= \\left[\\theta_{1,j}^{x_{j}}(1-\\theta_{1,j})^{1-x_{j}}\\right]^{y}\\left[\\theta_{0,j}^{x_{j}}(1-\\theta_{0,j})^{1-x_{j}}\\right]^{1-y}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Naive Bayes for spam classification -- likelihood\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\loglik(\\theta,\\pi\\given X,\\yy)=& \\underbrace{\\sum_{i=1}^N}_{\\textrm{samples}} \\left[ y_i\\log \\pi \n",
    "+ (1-y_i)\\log (1-\\pi) \\right] \\\\\n",
    "+& \\underbrace{\\sum_{i=1}^N}_{\\textrm{samples}}\n",
    "\\underbrace{\\sum_{j=1}^W}_{\\textrm{words}}  \n",
    "y_i x_{i,j}\\log\\theta_{1,j} \\\\\n",
    "+& \\underbrace{\\sum_{i=1}^N}_{\\textrm{samples}}\n",
    "\\underbrace{\\sum_{j=1}^W}_{\\textrm{words}}  \n",
    "y_i (1-x_{i,j})\\log(1-\\theta_{1,j}) \\\\\n",
    "+& \\underbrace{\\sum_{i=1}^N}_{\\textrm{samples}} \n",
    "\\underbrace{\\sum_{j=1}^W}_{\\textrm{words}}  \n",
    "(1-y_i) x_{i,j}\\log(\\theta_{0,j}) \\\\\n",
    "+& \\underbrace{\\sum_{i=1}^N}_{\\textrm{samples}}\n",
    "\\underbrace{\\sum_{j=1}^W}_{\\textrm{words}}  \n",
    "(1-y_i) (1-x_{i,j})\\log(1-\\theta_{0,j}) \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "Work out derivatives and updates for $\\pi_1$ and $\\theta_1,j$ on the board\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes for spam classification -- learning\n",
    "\n",
    "Given a training data with $N$ labeled e-mails, training a Naive Bayes spam model is accomplished using\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\pi_1 &= \\frac{\\sum_{i=1}^N [y_i = 1]}{N} && \\textrm{frequency of spam e-mail in the training data}\\\\\n",
    "\\theta_{1,j} &=  \\frac{\\sum_{i=1}^N x_{i,j}*[y_i=1]}{\\sum_{i=1}^N [y_i=1]} && \\textrm{freqency of word $j$ in spam e-mail}\\\\\n",
    "\\theta_{0,j} &=  \\frac{\\sum_{i=1}^N x_{i,j}*[y_i=0]}{\\sum_{i=1}^N [y_i=0]} && \\textrm{frequency of word $j$ in non-spam e-mail}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Naive Bayes for spam classification -- prediction\n",
    "Prediction for a new e-mail given its bag-of-words representation $\\xx$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(y = 1 \\given \\xx) &= \\frac{p(y=1, \\xx)}{p(\\xx)} = \\frac{p(y=1, \\xx)}{p(y=1,\\xx) + p(y=0,\\xx)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "If interested only in the most likely label of a message represented by $\\xx$\n",
    "$$\n",
    "\\argmax_c \\log p(y=c, \\xx) = \\argmax_c \\log\\pi_c + \\sum_{j=1}^W  \\left[ x_{j}\\log\\theta_{c,j} + (1-x_{j})\\log(1 - \\theta_{c,j}) \\right]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spam filtering -- smoothing\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\pi_1 &= \\frac{\\sum_{i=1}^N [y_i = 1]}{N} && \\textrm{frequency of spam e-mail in the training data}\\\\\n",
    "\\theta_{1,j} &=  \\frac{\\sum_{i=1}^N x_{i,j}*[y_i=1]}{\\sum_{i=1}^N  [y_i = 1]} && \\textrm{freqency of word $j$ in spam e-mail}\\\\\n",
    "\\theta_{0,j} &=  \\frac{\\sum_{i=1}^N x_{i,j}*[y_i=0]}{\\sum_{i=1}^N [y_i = 0]} && \\textrm{frequency of word $j$ in non-spam e-mail}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Q: What is the value of $\\theta_{1,j}$ if the word has never been seen in a spam e-mail in the dataset? Is this a problem? If so, why and how would you fix it?\n",
    "\n",
    "http://spamassassin.apache.org/\n",
    "\n",
    "Uses Naive Bayes approach with smoothing -- even though a word has not been seen it does not have probability 0.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hyperparameters\n",
    "\n",
    "In penalized regression we added ridge term\n",
    "$$\n",
    "\\loglik(\\beta) - \\frac{\\lambda}{2} \\sum_{j>0}\\beta_j^2\n",
    "$$\n",
    "\n",
    "Q: How do we choose $\\lambda$? What happens if we try to maximize penalized log-likelihood with respect to $\\lambda$?\n",
    "\n",
    "\n",
    "Q: How do we choose which words to count in our document classifiers? All of them? Do we have enough samples?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hyperparameters\n",
    "\n",
    "Hyperparameters, unlike parameters, typically constrain the model complexity to avoid overfitting.\n",
    "\n",
    "Overfitting occurs when our performance on training set is optimistic compared to performance on test set\n",
    "\n",
    "![](tradeoff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How do we select hyperparameters?\n",
    "\n",
    "One approach is to split the dataset into three parts:\n",
    "1. Training\n",
    "2. Validation\n",
    "3. Test\n",
    "\n",
    "Training subset is used to train models for different settings of hyperparameters\n",
    "for example $\\lambda \\in \\{0.1,0.001,0.0001..\\}$\n",
    "\n",
    "Validation subset is used to evaluate accuracy of the different models we trained and\n",
    "select the best model.\n",
    "\n",
    "Test set is used to compute the accuracy of the model selected on validation set.\n",
    "\n",
    "Q: Why can't we just report accuracy of the best model on the validation set? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How do we select hyperparameters if the dataset is small?\n",
    "\n",
    "Splitting data into three parts makes sense on a large dataset, but can hurt your performance on a small dataset.\n",
    "\n",
    "We can't look at the test set during training, but validation set seems to be just sitting around\n",
    "\n",
    "Q: Can we somehow use more of the training and validation data for training? What is the problem of leaving just one sample for validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The idea: cross-validation\n",
    "\n",
    "First take out test data, then split the remaining data into $k$ parts and treat each part as validation while training on the rest.\n",
    "\n",
    "An example of $4$-fold cross validation:\n",
    "\n",
    "0. Split data into disjoint subsets $\\Data = \\Data_1 \\cup \\Data_2 \\cup \\Data_3 \\cup \\Data_4$\n",
    "1. Train on $\\Data_2 \\cup\\Data_3\\cup\\Data_4$ compute $\\textrm{Error}_1$ on $\\Data_1$ \n",
    "2. Train on $\\Data_1\\cup\\Data_3\\cup\\Data_4$ compute $\\textrm{Error}_2$ on $\\Data_2$ \n",
    "3. Train on $\\Data_1\\cup\\Data_2\\cup\\Data_4$ compute $\\textrm{Error}_3$ on $\\Data_3$ \n",
    "4. Train on $\\Data_1\\cup\\Data_2\\cup\\Data_3$ compute $\\textrm{Error}_4$ on $\\Data_4$ \n",
    "\n",
    "![](cv.png)\n",
    "\n",
    "Report \n",
    "$$\n",
    "\\textrm{CVError} = \\frac{\\textrm{Error}_1 + \\textrm{Error}_2 + \\textrm{Error}_3 + \\textrm{Error}_4}{4}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# k-fold cross-validation for linear regression\n",
    "\n",
    "![](cvlinreg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification performance -- confusion matrix\n",
    "\n",
    "Say we trained a classifier and want to see how well it works.\n",
    "\n",
    "We can report a single number, accuracy, that tells us how often it is right.\n",
    "\n",
    "However, this hides information about where the classifier fails.\n",
    "\n",
    "Confusion matrix is given by:\n",
    "\n",
    "|Predicted \\ True     | $y=1$ | $y=0$   |\n",
    "| ------ | ------ | ------ |\n",
    "| $\\hat{y}=1$ | True Positive  | False Positive |\n",
    "| $\\hat{y}=0$ | False Negative | True Positive |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification performance -- prediction rates\n",
    "\n",
    "Prediction rates\n",
    "* true positive rate $$ TPR = \\frac{TP}{TP + FN} $$\n",
    "* false positive rate $$ FPR = \\frac{FP}{TN + FP} $$\n",
    "* true negative rate $$ TNR = \\frac{TN}{TN + FP} $$\n",
    "* false negative rate $$ FNR = \\frac{FN}{TP + FN} $$\n",
    "\n",
    "Note that $TP + FN$ is total number of positive examples. \n",
    "\n",
    "Similarly $TN + FP$ is total number of negative examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification performance -- ROC curves\n",
    "\n",
    "Predictions are based on a cutoff\n",
    "$$\n",
    "p(y=1|\\xx)>\\tau\n",
    "$$\n",
    "where $\\tau$ is typically 0.5.\n",
    "\n",
    "This particular cutoff will result in a specific prediction rates.\n",
    "\n",
    "However, you may prefer to tradeoff false positives for false negatives -- health industry does.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification performance -- ROC curves\n",
    "\n",
    "![](roc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Today\n",
    "* Naive Bayes example\n",
    "* Hyperaparameter selection\n",
    "* K-fold Cross-validation \n",
    "* Prediction rates, confusion matrix and ROC\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
